package lsm

import (
	"context"
	"fmt"
	"os"
	"path/filepath"
	"sync"
	"sync/atomic"
	"time"

	"kv/pkg/cache"
	"kv/pkg/common"
	"kv/pkg/config"
	"kv/pkg/iterator"
	"kv/pkg/logger"
	"kv/pkg/memtable"
	"kv/pkg/sst"
	"kv/pkg/utils"
	"kv/pkg/wal"
)

// Engine ä»£è¡¨ LSM æ ‘å­˜å‚¨å¼•æ“Ž
type Engine struct {
	//æ ¸å¿ƒéƒ¨ä»¶
	config      *config.Config
	dataDir     string
	memTable    *memtable.MemTable
	blockCache  *cache.BlockCache
	fileManager *utils.FileManager

	// SST management
	levels   *LevelManager
	metadata *EngineMetadata
	// WAL management
	wal *wal.WAL

	// Metadata persistence
	metadataFile string

	// Background workers
	checkCh chan struct{}

	wg sync.WaitGroup

	// Statistics
	stats *EngineStatistics

	txnManager *TransactionManager

	// State
	closed                bool
	flushAndCompactByHand bool // during test, disable background flush and compact to make it easy to debug

	// æŽ§åˆ¶åŽå° goroutine çš„ä¸Šä¸‹æ–‡
	ctx    context.Context
	cancel context.CancelFunc
}

// init
func (e *Engine) initTxnManager(config *TransactionConfig) error {
	if config == nil {
		// è¿”å›ž é»˜è®¤é…ç½®
		config = DefaultTransactionConfig()
	}
	// åˆ›å»ºä¸€ä¸ªæ–°çš„äº‹åŠ¡ç®¡ç†å™¨
	manger := NewTransactionManager(e, config)
	e.txnManager = manger
	return nil
}

// NewEngine creates a new LSM engine
func NewEngine(cfg *config.Config, dataDir string) (*Engine, error) {
	// é…ç½®æ˜¯å¦æ­£ç¡®
	if err := cfg.Validate(); err != nil {
		return nil, fmt.Errorf("invalid config: %w", err)
	}
	// å¦‚æžœæ•°æ®ç›®å½•ä¸å­˜åœ¨ï¼Œåˆ™åˆ›å»ºæ•°æ®ç›®å½•ã€‚()
	if err := os.MkdirAll(dataDir, 0755); err != nil {
		return nil, fmt.Errorf("failed to create data directory: %w", err)
	}

	// Initialize file manager æ–‡ä»¶ç®¡ç†
	fileManager := utils.NewFileManager(dataDir)

	// Initialize block cache å—ç®¡ç†cache
	blockCache := cache.NewBlockCache(cfg.GetBlockCacheCapacity())

	// Initialize memtable åˆå§‹åŒ–å†…å­˜è¡¨
	mt := memtable.New()

	// Initialize level manager åˆå§‹åŒ–çº§åˆ«ç®¡ç†å™¨
	levels := NewLevelManager(cfg, fileManager, blockCache)

	// Initialize WAL
	walDir := filepath.Join(dataDir, "wal")
	walConfig := &wal.Config{
		LogDir:        walDir,
		BufferSize:    int(cfg.GetWALBufferSize() / 64), // Convert bytes to record count estimate
		FileSizeLimit: cfg.GetWALFileSizeLimit(),
		CleanInterval: time.Duration(cfg.GetWALCleanInterval()) * time.Second,
	}

	walInstance, err := wal.New(walConfig, 0) // ä»Žæ£€æŸ¥ç‚¹ 0 å¼€å§‹
	if err != nil {
		return nil, fmt.Errorf("failed to initialize WAL: %w", err)
	}

	engine := &Engine{
		config:      cfg,         // å¼•æ“Žé…ç½®ï¼ˆå¦‚å±‚çº§æ¯”ä¾‹ã€SSTå¤§å°ç­‰ï¼‰
		dataDir:     dataDir,     // æ•°æ®æ ¹ç›®å½•ï¼ˆæ‰€æœ‰æ–‡ä»¶çš„åŸºç¡€è·¯å¾„ï¼‰
		memTable:    mt,          // å†…å­˜è¡¨ï¼ˆMemTableï¼ŒæŽ¥æ”¶å®žæ—¶å†™å…¥çš„æœ‰åºæ•°æ®ç»“æž„ï¼‰
		blockCache:  blockCache,  // æ•°æ®å—ç¼“å­˜ï¼ˆåŠ é€ŸSSTæ–‡ä»¶çš„è¯»æ“ä½œï¼‰
		fileManager: fileManager, // æ–‡ä»¶ç®¡ç†å™¨ï¼ˆç»Ÿä¸€å¤„ç†æ–‡ä»¶IOï¼‰
		levels:      levels,      // LSMæ ‘çš„å±‚çº§é›†åˆï¼ˆåˆå§‹å¯èƒ½ä¸ºç©ºï¼ŒåŽç»­æ¢å¤æˆ–åˆ›å»ºï¼‰
		wal:         walInstance, // WALæ—¥å¿—å®žä¾‹ï¼ˆé¢„å†™æ—¥å¿—ï¼Œä¿è¯æ•°æ®æŒä¹…åŒ–ï¼‰
		metadata: &EngineMetadata{ // å¼•æ“Žå…ƒæ•°æ®ï¼ˆæ ¸å¿ƒIDç”Ÿæˆå™¨çš„åˆå§‹å€¼ï¼‰
			NextSSTID:       0, // ä¸‹ä¸€ä¸ªSSTæ–‡ä»¶IDï¼ˆä»Ž0å¼€å§‹è‡ªå¢žï¼‰
			NextTxnID:       1, // ä¸‹ä¸€ä¸ªäº‹åŠ¡IDï¼ˆä»Ž1å¼€å§‹ï¼Œ0é€šå¸¸ä¸ºç‰¹æ®Šæ ‡è¯†ï¼‰
			GlobalReadTxnID: 1, // å…¨å±€è¯»äº‹åŠ¡ç‰ˆæœ¬å·ï¼ˆåˆå§‹å¯è§ç‰ˆæœ¬ï¼‰
		},
		stats:        &EngineStatistics{},                // å¼•æ“Žç»Ÿè®¡ä¿¡æ¯ï¼ˆå¦‚è¯»å†™æ¬¡æ•°ã€åˆå¹¶æ¬¡æ•°ç­‰ï¼‰
		metadataFile: filepath.Join(dataDir, "metadata"), // å…ƒæ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆæŒä¹…åŒ–å…ƒæ•°æ®ï¼‰
		closed:       false,                              // å¼•æ“Žæ˜¯å¦å·²å…³é—­ï¼ˆåˆå§‹ä¸ºæœªå…³é—­ï¼‰
		checkCh:      make(chan struct{}, 1),             // ç”¨äºŽè§¦å‘æ£€æŸ¥çš„é€šé“ï¼ˆå¦‚æ£€æŸ¥æ˜¯å¦éœ€è¦åˆå¹¶ï¼‰
	}
	engine.initTxnManager(nil)

	// Create context for background workers
	engine.ctx, engine.cancel = context.WithCancel(context.Background())

	// Load metadata if exists
	if err := loadMetadata(engine); err != nil {
		logger.Warnf("failed to load metadata: %v", err)
	}

	// Recover from existing data if any
	if err := engine.recover(); err != nil {
		return nil, fmt.Errorf("recovery failed: %w", err)
	}

	// Save initial metadata
	if err := saveMetadata(engine); err != nil {
		return nil, fmt.Errorf("failed to save initial metadata: %w", err)
	}

	// Start background workers
	engine.startBackgroundWorkers()

	return engine, nil
}

// recover ä»Žç£ç›˜æ¢å¤å¼•æ“ŽçŠ¶æ€
func (e *Engine) recover() error {
	// First, recover SST files
	if err := e.levels.LoadExistingSSTs(); err != nil {
		return fmt.Errorf("failed to load existing SST files: %w", err)
	}

	// Then, recover from WAL
	if e.wal != nil {
		if err := e.recoverFromWAL(); err != nil {
			return fmt.Errorf("failed to recover from WAL: %w", err)
		}
	}

	return nil
}

// recoverFromWAL ä»Ž WAL æ—¥å¿—ä¸­æ¢å¤æœªæäº¤çš„äº‹åŠ¡
func (e *Engine) recoverFromWAL() error {
	// Read WAL records
	walDir := filepath.Join(e.dataDir, "wal")
	recordsByTxn, err := wal.Recover(walDir, 0) // Recover from checkpoint 0
	if err != nil {
		return fmt.Errorf("failed to read WAL records: %w", err)
	}

	if len(recordsByTxn) == 0 {
		return nil // No records to recover
	}

	logger.Infof("ðŸ”„ Check %d transactions from WAL...\n", len(recordsByTxn))

	// Process each transaction
	hasRepayed := false
	for txnID, records := range recordsByTxn {
		if e.txnManager.needRepay(txnID) {
			if err := e.replayTransaction(txnID, records); err != nil {
				logger.Errorf("Warning: failed to replay transaction %d: %v\n", txnID, err)
				os.Exit(1)
			}
			hasRepayed = true
			logger.Infof(" âœ… Replayed record %+v.\n", records)
		}
	}
	if hasRepayed {
		logger.Infof("âœ… WAL recovery completed. Next transaction ID: %d\n", e.metadata.NextTxnID)
	} else {
		logger.Info("âœ… WAL recovery completed. No transactions to replay.")
	}

	return nil
}

// replayTransaction replays a single transaction from WAL records
func (e *Engine) replayTransaction(txnID uint64, records []*wal.Record) error {
	if len(records) == 0 {
		return nil
	}

	// Check if transaction was committed or rolled back
	var committed, rolledBack bool
	for _, record := range records {
		switch record.OpType {
		case wal.OpCommit:
			committed = true
		case wal.OpRollback:
			rolledBack = true
		}
	}

	// If transaction was committed, replay all operations
	if committed {
		logger.Debugf("  Replaying committed transaction %d...\n", txnID)
		for _, record := range records {
			switch record.OpType {
			case wal.OpPut:
				if err := e.memTable.Put(record.Key, record.Value, txnID); err != nil {
					return fmt.Errorf("failed to replay PUT %s: %w", record.Key, err)
				}
			case wal.OpDelete:
				if err := e.memTable.Delete(record.Key, txnID); err != nil {
					return fmt.Errorf("failed to replay DELETE %s: %w", record.Key, err)
				}
			}
		}
		return nil
	}

	// If transaction was rolled back or incomplete, ignore it
	if rolledBack {
		logger.Debugf("  Skipping rolled back transaction %d\n", txnID)
	} else {
		logger.Debugf("  Skipping incomplete transaction %d\n", txnID)
	}

	return nil
}

// startBackgroundWorkers starts the background flush and compaction workers
func (e *Engine) startBackgroundWorkers() {
	// Flush worker
	e.wg.Add(3)
	go e.flushWorker()
	go e.cleanWalWorker()
	go e.syncTxnStatusWorker()

	if e.config.Compaction.EnableAutoCompaction {
		e.wg.Add(1)
		go e.compactionWorker()
	}
}

//////////////////////////////////////////////////////////////////////////////////////////

// Put inserts or updates a key-value pair
func (e *Engine) Put(key, value string) error {
	txnID := atomic.AddUint64(&e.metadata.NextTxnID, 1) - 1
	return e.PutWithTxnID(key, value, txnID)
}

// PutWithTxn inserts or updates a key-value pair with transaction ID
func (e *Engine) PutWithTxnID(key, value string, txnID uint64) error {
	if e.closed {
		return utils.ErrClosed
	}

	err := e.memTable.Put(key, value, txnID)
	if err != nil {
		return err
	}

	// Check if we need to freeze the current memtable
	if int64(e.memTable.GetCurrentSize()) >= e.config.GetPerMemSizeLimit() {
		e.freezeMemTableIfNeeded()
	}

	atomic.AddUint64(&e.stats.Writes, 1)
	atomic.StoreUint64(&e.stats.MemTableSize, uint64(e.memTable.GetCurrentSize()))
	atomic.StoreUint64(&e.stats.FrozenTableSize, uint64(e.memTable.GetFrozenSize()))

	return nil
}

// PutBatch inserts or updates multiple key-value pairs atomically
func (e *Engine) PutBatch(kvs []common.KVPair) error {
	txnID := atomic.AddUint64(&e.metadata.NextTxnID, 1) - 1
	return e.PutBatchWithTxnID(kvs, txnID)
}

// PutBatchWithTxn inserts or updates multiple key-value pairs with transaction ID
func (e *Engine) PutBatchWithTxnID(kvs []common.KVPair, txnID uint64) error {
	if e.closed {
		return utils.ErrClosed
	}

	err := e.memTable.PutBatch(kvs, txnID)
	if err != nil {
		return err
	}

	// Check if we need to freeze the current memtable
	if int64(e.memTable.GetCurrentSize()) >= e.config.GetPerMemSizeLimit() {
		e.freezeMemTableIfNeeded()
	}

	atomic.AddUint64(&e.stats.Writes, uint64(len(kvs)))
	atomic.StoreUint64(&e.stats.MemTableSize, uint64(e.memTable.GetCurrentSize()))
	atomic.StoreUint64(&e.stats.FrozenTableSize, uint64(e.memTable.GetFrozenSize()))

	return nil
}

// Get retrieves a value by key
func (e *Engine) Get(key string) (string, bool, error) {
	return e.GetWithTxnID(key, 0)
}

// GetWithTxnID retrieves a value by key with transaction ID for snapshot isolation
func (e *Engine) GetWithTxnID(key string, txnID uint64) (string, bool, error) {
	if e.closed {
		return "", false, utils.ErrClosed
	}

	atomic.AddUint64(&e.stats.Reads, 1)

	// First, check memtable (current + frozen) with transaction ID
	value, found, err := e.memTable.Get(key, txnID)
	if err != nil {
		return "", false, err
	}
	if found {
		return value, true, nil
	}

	// Then, check SST files from level 0 to highest level with transaction ID
	val, found, err := e.levels.Get(key, txnID)
	if err != nil {
		return "", false, err
	}
	if found && val == "" {
		return "", false, nil
	}
	return val, found, nil
}

// GetBatch retrieves multiple values by keys
func (e *Engine) GetBatch(keys []string) ([]memtable.GetResult, error) {
	return e.GetBatchWithTxnID(keys, 0)
}

// GetBatchWithTxn retrieves multiple values by keys with transaction ID
func (e *Engine) GetBatchWithTxnID(keys []string, txnID uint64) ([]memtable.GetResult, error) {
	if e.closed {
		return nil, utils.ErrClosed
	}

	defer atomic.AddUint64(&e.stats.Reads, uint64(len(keys)))

	results := make([]memtable.GetResult, len(keys))

	// First try to get all keys from memtable with transaction ID
	memResults, err := e.memTable.GetBatch(keys, txnID)
	if err != nil {
		return nil, err
	}

	// Copy memtable results and identify missing keys
	missingKeys := make([]string, 0)
	for i, result := range memResults {
		results[i] = result
		if !result.Found {
			missingKeys = append(missingKeys, result.Key)
		}
	}

	// Get missing keys from SST files with transaction ID
	if len(missingKeys) > 0 {
		sstResults, err := e.levels.GetBatch(missingKeys, txnID)
		if err != nil {
			return nil, err
		}

		// Merge SST results back into final results
		sstResultMap := make(map[string]memtable.GetResult)
		for _, result := range sstResults {
			sstResultMap[result.Key] = result
		}

		for i := range results {
			if !results[i].Found {
				if sstResult, found := sstResultMap[results[i].Key]; found {
					results[i] = sstResult
				}
			}
		}
	}

	return results, nil
}

func (e *Engine) Delete(key string) error {
	txnID := atomic.AddUint64(&e.metadata.NextTxnID, 1) - 1
	return e.DeleteWithTxnID(key, txnID)
}

// Delete marks a key as deleted
func (e *Engine) DeleteWithTxnID(key string, txnID uint64) error {
	if e.closed {
		return utils.ErrClosed
	}

	// Check if we need to freeze the current memtable
	if int64(e.memTable.GetCurrentSize()) >= e.config.GetPerMemSizeLimit() {
		e.freezeMemTableIfNeeded()
	}

	err := e.memTable.Delete(key, txnID)
	if err != nil {
		return err
	}

	atomic.AddUint64(&e.stats.Deletes, 1)
	atomic.StoreUint64(&e.stats.MemTableSize, uint64(e.memTable.GetCurrentSize()))
	atomic.StoreUint64(&e.stats.FrozenTableSize, uint64(e.memTable.GetFrozenSize()))

	return nil
}

// NewIterator creates a new iterator for scanning the database
func (e *Engine) NewIterator() iterator.Iterator {
	if e.closed {
		return iterator.NewEmptyIterator()
	}

	// Create merge iterator combining memtable and all SST levels
	iterators := make([]iterator.Iterator, 0)

	// Add memtable iterator
	memIter := e.memTable.NewIterator(0)
	iterators = append(iterators, memIter)

	// Add SST level iterators
	levelIters := e.levels.GetIterators(0)
	iterators = append(iterators, levelIters...)

	return iterator.NewMergeIterator(iterators)
}

// freezeMemTableIfNeeded freezes the current memtable if it's too large
func (e *Engine) freezeMemTableIfNeeded() {
	if int64(e.memTable.GetCurrentSize()) >= e.config.GetPerMemSizeLimit() {
		e.memTable.FreezeCurrentTable()
	}
}

// Flush forces a flush of frozen memtables to disk
func (e *Engine) Flush() error {
	if e.closed {
		return utils.ErrClosed
	}

	if !e.flushAndCompactByHand {
		return e.NoticeFlushCheck()
	}

	return e._doFlush()
}

func (e *Engine) updateOnTxnFlushed(entry *iterator.Entry) error {
	e.txnManager.updateFlushedTxn(entry.TxnID)
	return nil
}

// _doFlush performs the actual flush operation
// need lock outside
func (e *Engine) _doFlush() error {
	flushResult, err := e.memTable.FlushOldest()
	if err != nil {
		return fmt.Errorf("flush failed: %w", err)
	}

	if flushResult == nil {
		return nil // Nothing was flushed
	}

	// Build SST from flush result
	sstID := atomic.AddUint64(&e.metadata.NextSSTID, 1) - 1
	sstPath := e.fileManager.GetSSTPath(sstID, 0) // Level 0

	builder := sst.NewSSTBuilder(e.config.GetBlockSize(), true) // Enable bloom filter

	// Add entries from flush result
	for _, entry := range flushResult.Entries {
		if entry.Key == "" && entry.Value == "" {
			e.updateOnTxnFlushed(&entry)
			// skip the transaction end marker
			continue
		}
		err := builder.Add(entry.Key, entry.Value, entry.TxnID)
		if err != nil {
			return fmt.Errorf("failed to add entry to SST builder: %w", err)
		}
	}

	// Build the SST file
	newSST, err := builder.Build(sstID, sstPath, e.blockCache)
	if err != nil {
		return fmt.Errorf("failed to build SST: %w", err)
	}

	// Add the new SST to level 0
	err = e.levels.AddSST(0, newSST)
	if err != nil {
		newSST.Close()
		newSST.Delete()
		return fmt.Errorf("failed to add SST to level manager: %w", err)
	}

	// Save metadata after updating nextSSTID
	if err := saveMetadata(e); err != nil {
		logger.Errorf("Warning: failed to save metadata after flush: %v\n", err)
	}

	// Update statistics
	atomic.AddUint64(&e.stats.Flushes, 1)
	atomic.AddUint64(&e.stats.FlushedBytes, uint64(newSST.Size()))
	atomic.StoreUint64(&e.stats.FrozenTableSize, uint64(e.memTable.GetFrozenSize()))

	return nil
}

// flushWorker runs in the background to flush memtables when needed
func (e *Engine) flushWorker() {
	defer e.wg.Done()

	if e.flushAndCompactByHand {
		return
	}

	for {
		select {
		case <-e.ctx.Done():
			return
		case <-e.checkCh:
		case <-utils.After(200): // 200ms
			// Check if we need to flush
			if e.memTable.CanFlush() {
				if err := e._doFlush(); err != nil {
					// Log error but continue
					logger.Errorf("Background flush error: %v\n", err)
				}
			}
		}
	}
}

func (e *Engine) flushAtClose() error {
	for !e.memTable.Empty() {
		if err := e._doFlush(); err != nil {
			return err
		}
	}
	return nil
}

// compactionWorker runs in the background to compact SSTs when needed
func (e *Engine) compactionWorker() {
	defer e.wg.Done()

	if e.flushAndCompactByHand {
		return
	}

	for {
		select {
		case <-e.ctx.Done():
			return
		case <-utils.After(5000): // 5 seconds
			// Check if automatic compaction is needed
			if e.levels.NeedsCompaction() {
				if err := e.doCompaction(); err != nil {
					// Log error but continue
					logger.Errorf("Background compaction error: %v\n", err)
				}
			}
		}
	}
}

// cleanupLoop runs in a background goroutine to clean old WAL files
func (e *Engine) cleanWalWorker() {
	defer e.wg.Done()
	logger.Infof("Starting WAL cleanup loop\n")

	ticker := time.NewTicker(time.Duration(e.config.WAL.CleanInterval))
	defer ticker.Stop()

	for {
		select {
		case <-e.ctx.Done():
			return
		case <-ticker.C:
			activeTxnIDs := e.txnManager.GetactiveTxnIDs()
			e.wal.CleanOldFiles(activeTxnIDs)
		}
	}
}
func (e *Engine) NoticeFlushCheck() error {
	if e.closed {
		return utils.ErrClosed
	}
	select {
	case e.checkCh <- struct{}{}:
	default:
		return nil
	}
	return nil
}

func (e *Engine) ForceCompact() {
	if e.closed {
		return
	}

	if !e.flushAndCompactByHand {
		return
	}
	// Manual compaction trigger
	if err := e.doCompaction(); err != nil {
		// Log error but continue
		logger.Errorf("Manual compaction error: %v\n", err)
	}
}

// doCompaction performs compaction
func (e *Engine) doCompaction() error {
	task := e.levels.PickCompactionTask()
	if task == nil {
		return nil // No compaction needed
	}

	return e.levels.ExecuteCompaction(task, e.metadata.NextSSTID, &e.metadata.NextSSTID)
}

func (e *Engine) compactAtClose() error {
	for {
		task := e.levels.PickCompactionTask()
		if task == nil {
			return nil // No compaction needed
		}
		if err := e.levels.ExecuteCompaction(task, e.metadata.NextSSTID, &e.metadata.NextSSTID); err != nil {
			return err
		}
	}
}

// ForceCompaction forces a compaction of the specified level
func (e *Engine) ForceCompaction(level int) error {
	if e.closed {
		return utils.ErrClosed
	}

	task := e.levels.CreateCompactionTask(level)
	if task == nil {
		return nil // Nothing to compact
	}

	return e.levels.ExecuteCompaction(task, e.metadata.NextSSTID, &e.metadata.NextSSTID)
}

// cleanupWorker runs periodically to clean up old committed transactions
func (e *Engine) syncTxnStatusWorker() {
	defer e.wg.Done()

	ticker := time.NewTicker(e.txnManager.config.CleanupInterval)
	defer ticker.Stop()

	for {
		select {
		case <-e.ctx.Done():
			return
		case <-ticker.C:
			e.txnManager.syncTxnStatus()
		}
	}
}

// GetStatistics returns engine statistics
func (e *Engine) GetStatistics() EngineStatistics {
	return EngineStatistics{
		Reads:           atomic.LoadUint64(&e.stats.Reads),
		Writes:          atomic.LoadUint64(&e.stats.Writes),
		Deletes:         atomic.LoadUint64(&e.stats.Deletes),
		Flushes:         atomic.LoadUint64(&e.stats.Flushes),
		FlushedBytes:    atomic.LoadUint64(&e.stats.FlushedBytes),
		Compactions:     atomic.LoadUint64(&e.stats.Compactions),
		CompactedBytes:  atomic.LoadUint64(&e.stats.CompactedBytes),
		CompactedFiles:  atomic.LoadUint64(&e.stats.CompactedFiles),
		MemTableSize:    atomic.LoadUint64(&e.stats.MemTableSize),
		FrozenTableSize: atomic.LoadUint64(&e.stats.FrozenTableSize),
		SSTFiles:        e.levels.GetTotalSSTCount(),
		TotalSSTSize:    e.levels.GetTotalSSTSize(),
	}
}

func (e *Engine) GetMeta() *EngineMetadata {
	return e.metadata
}

// GetLevelInfo returns information about all levels
func (e *Engine) GetLevelInfo() []LevelInfo {

	return e.levels.GetLevelInfo()
}

// GetWAL returns the WAL instance
func (e *Engine) GetWAL() *wal.WAL {
	return e.wal
}

// DeleteWithTxn marks a key as deleted with transaction ID
func (e *Engine) DeleteWithTxn(key string, txnID uint64) error {
	if e.closed {
		return utils.ErrClosed
	}

	// Check if we need to freeze the current memtable
	if int64(e.memTable.GetCurrentSize()) >= e.config.GetPerMemSizeLimit() {
		e.freezeMemTableIfNeeded()
	}

	err := e.memTable.Delete(key, txnID)
	if err != nil {
		return err
	}

	atomic.AddUint64(&e.stats.Deletes, 1)
	atomic.StoreUint64(&e.stats.MemTableSize, uint64(e.memTable.GetCurrentSize()))
	atomic.StoreUint64(&e.stats.FrozenTableSize, uint64(e.memTable.GetFrozenSize()))

	return nil
}

// NewIteratorWithTxnID creates a new iterator with transaction ID for snapshot isolation
func (e *Engine) NewIteratorWithTxnID(txnID uint64) iterator.Iterator {
	if e.closed {
		return iterator.NewEmptyIterator()
	}

	// Create merge iterator combining memtable and all SST levels with transaction ID
	iterators := make([]iterator.Iterator, 0)

	// Add memtable iterator with transaction ID
	memIter := e.memTable.NewIterator(txnID)
	iterators = append(iterators, memIter)

	// Add SST level iterators with transaction ID
	levelIters := e.levels.GetIterators(txnID)
	iterators = append(iterators, levelIters...)

	return iterator.NewMergeIterator(iterators)
}

// Close closes the engine and releases resources
func (e *Engine) Close() error {
	if e.closed {
		return nil
	}

	e.closed = true

	e.cancel()
	e.wg.Wait()

	e.flushAtClose()
	e.compactAtClose()

	// Save metadata before shutdown
	if err := saveMetadata(e); err != nil {
		logger.Errorf("Warning: failed to save metadata during shutdown: %v\n", err)
	}

	e.txnManager.Close()
	e.wal.Close()

	return nil
}
